# Barriers to reproducibility

So far we have described the [importance](../01/importantforscience) of reproducible research and motivated why we think [you should care](../02/whycare).

But there are many barriers to reproducible research.
You can watch Kirstie Whitaker describe some of them in [her talk about _The Turing Way_](https://youtu.be/wZeoZaIV0VE?t=312) at [csv,conf,v4](https://csvconf.com/2019) in May 2019.
You can use and re-use her slides under a CC-BY licence via Zenodo (doi: [10.5281/zenodo.2669547](https://doi.org/10.5281/zenodo.2669547)).
The section describing the slide below starts around 5 minutes into the video.

| ![Barriers to reproducible research](../../figures/reproducibility/barriers.png) |
| -------------------------------------------------------------------------------------------------------- |
|  Some of the barriers to reproducible research. |

This chapter outlines some of those barriers, and a couple of suggestions to get around them.

## Plead the fifth 

No-one wants to incriminate themselves, and no-one is infallible. Putting your code online can be very revealing and intimidating, and people are scared of being judged by others. However, releasing code can help other researchers provide feedback, learn and may help them in their research. Publishing your code encourages you to write your code to a high standard and can help generate new ideas. **Let the person who is without bugs report the first error.**

## Publication bias towards novel findings


Novel results are not necessarily accurate or interesting but they are rewarded in the academic world!
Papers that do not find statistically significant relationships are hard to publish, particularly if the results *do not* reproduce previously published findings.
(That includes statistically significant findings that go in the opposite direction to already published work.)
Similarly, an article might be less likely to be accepted to a journal or a confence if it successfully reproduces already-published results instead of producing a new set.
There's a good chance that reviewers will say "we already know this" and reject the submission.

John Ioannidis published an influential paper in 2005 titled "Why Most Published Research Findings Are False" (doi: [10.1371/journal.pmed.0020124](https://doi.org/10.1371/journal.pmed.0020124)) which discusses the many factors that contribute to publication bias.
Therefore it is very likely that there is a lot of duplicated work in data science.
Too many different researchers are asking the same question, not getting the answer they expect or want, and then not telling anyone what they have found.

## Barrier 3

*replace this text with the content of barrier 3*

## Barrier 4

*replace this text with the content of barrier 4*

## Barrier 5

*replace this text with the content of barrier 5*

## Barrier 6

Takes Time

The time needed to maintain a reproducible project presents a significant barrier. At the start of the project considerable time needs to be invested in designing and setting up the reproducibility pipepine. This may include a testing framework , continuous integration, Github repository and data rights. Time may also be spent communicating with collaborators to agree which parts of the project may be open source and how they are shared, this may include creating synthetic data.Throughout the project, considerable time needs to be spent mainting the reprodicibility pipeline including refactoring, testing and resolving conflicts. At project conclusion time usually spent modifying analysis may be saved thanks to the reproducibility pipeline. However, time may be lost at review due to framework explanation and suggested adaptations.


## Barrier 7

*replace this text with the content of barrier 7*

## Barrier 8

*replace this text with the content of barrier 8*

## Barrier 9

*replace this text with the content of barrier 9*
